# Speech Commands Data Set

## About Data Set
[Speech Commands Data Set](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) is a set of one-second .wav audio files, each containing a single spoken English word.
These words are from a small set of commands, and are spoken by a variety of different speakers.
The audio files are organized into folders based on the word they contain, and this data set is designed to help train simple machine learning models.

## Data Preparation

In order to be able to train and evaluate models on Speech Commands Data Set using our code base, we have to download Speech Commands Data Set v0.01 and organize data to three exclusive splits:

* `train`
* `valid`
* `test`

We offer automated script (`download_and_split.sh`) which will download and process into predefined format.
```bash
bash download_and_split.sh [/path/for/dataset]
```

## Advanced Usage
Below is a detail description of how `download_and_split.sh` works.

### 1. Download Speech Commands Data Set

```bash
work_dir=${1:-$(pwd)/google_speech_commands}
mkdir -p ${work_dir}
pushd ${work_dir}
wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz
tar xzf speech_commands_v0.01.tar.gz
popd
```

### 2. Modify Data Set File Structure

`speech_commands_dataset` contains three files `train.txt`, `valid.txt` and `test.txt` composed of utterance file paths associated with data set splits.

Our codebase requires file structure for training, validation and test data such as follows:

```
─ v0.01_split
  ├── train
  │   ├── _background_noise_
  │   ├── down
  │   ├── go
  │   ├── left
  │   ├── no
  │   ├── off
  │   ├── on
  │   ├── right
  │   ├── stop
  │   ├── unknown
  │   ├── up
  │   └── yes
  ├── valid
  │   ├── _background_noise_
  │   ├── down
  │   ├── go
  │   ├── left
  │   ├── no
  │   ├── off
  │   ├── on
  │   ├── right
  │   ├── stop
  │   ├── unknown
  │   ├── up
  │   └── yes
  └── test
      ├── _background_noise_
      ├── down
      ├── go
      ├── left
      ├── no
      ├── off
      ├── on
      ├── right
      ├── stop
      ├── unknown
      ├── up
      └── yes
```

To convert Speech Commands Data Set to structure shown above run the command below.
This script does not copy any utterance files, only creates symlinks to the original dataset.

```bash
output_dir=${work_dir}/splitted_data
python google_speech_commmands_dataset_to_our_format_with_split.py \
    --input_dir `realpath ${work_dir}` \
    --train_list_fullpath train.txt \
    --valid_list_fullpath valid.txt \
    --test_list_fullpath test.txt \
    --wanted_words yes,no,up,down,left,right,on,off,stop,go \
    --output_dir `realpath ${output_dir}`
```

`output_dir` will be used for further train and evaluation.

## Why don't we use script provided by Tensorflow for data split?

TL;DR Script does not create deterministic split even with fixed random seed.

Tensorflow provides [preprocessing scripts](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands) for Speech Commands Data Set, however while [creating `unknown_index` dictionary](https://github.com/tensorflow/tensorflow/blob/d75adcad0d74e15893a93eb02bed447ec7971664/tensorflow/examples/speech_commands/input_data.py#L280-L294) nondeterministic ordering of utterance files in `unknown_index` comes into play.
[`prepare_data_index` method is using only limited number of words](https://github.com/tensorflow/tensorflow/blob/d75adcad0d74e15893a93eb02bed447ec7971664/tensorflow/examples/speech_commands/input_data.py#L315-L316) from `unknown_index` and assumes that the order of directories and files sought with `gfile.Glob(search_path)` is deterministic on any platform.
Unfortunately, the ordering of directories and files returned by `gfile.Glob(search_path)` is given by `directory order` (can be shown with [`ls -U`](https://unix.stackexchange.com/questions/13451/what-is-the-directory-order-of-files-in-a-directory-used-by-ls-u) command) and does not ensure the same ordering for everybody.

Because we want to ensure that anybody can reproduce our results we provide train, valid, and test split in `train.txt`, `valid.txt`, and `test.txt`, which were generated by Tensorflow's preprocessing scripts in our environment.
